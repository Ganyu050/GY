def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, theta):
    m = len(y)
    predictions = sigmoid(X.dot(theta))
    cost = (-1/m) * (y.T.dot(np.log(predictions)) + (1 - y).T.dot(np.log(1 - predictions)))
    return cost

def gradient_descent_logistic(X, y, theta, learning_rate=0.01, iterations=1000):
    m = len(y)
    for i in range(iterations):
        predictions = sigmoid(X.dot(theta))
        errors = predictions - y
        gradient = (1/m) * X.T.dot(errors)
        theta = theta - learning_rate * gradient
    return theta

# 生成一些示例数据
X = np.random.rand(100, 2)
y = (np.random.rand(100) > 0.5).astype(int).reshape(-1, 1)

# 添加偏置项
X_b = np.c_[np.ones((X.shape[0], 1)), X]

# 初始化参数
theta = np.zeros(X_b.shape[1])

# 梯度下降
theta_best = gradient_descent_logistic(X_b, y, theta)

print("通过逻辑回归梯度下降法得到的参数：", theta_best)

# 我觉得逻辑回归的梯度下降法实现起来比线性回归稍微复杂一些，因为需要使用sigmoid函数来处理分类问题。
# 不过，基本的思路还是类似的，都是通过梯度下降来最小化损失函数。